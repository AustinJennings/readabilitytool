{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGIS 501 Final Project - Web Scraping and Natural Language Processing\n",
    "\n",
    "In this project file, I use lxml, BeautifulSoup, Pandas, and textstat libraries to pull a list of all available page URLs in the domain www.tpchd.org, and then parse through the main content area of each and evaluate the reading level.  The end product is a csv file that contains the URL and reading level of all 330+ pages. \n",
    "<br>\n",
    "Runtime is about five and a half (5.5) minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************Assessment of page readability is complete.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "import requests                # for getting the XML sitemap from tpchd.org\n",
    "from requests import get       # for calling HTML objects from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas                  # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam server requests (can slow rate)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "\n",
    "# Open sitemap showing all pages from URL to xml\n",
    "sitereq = requests.request('GET', 'http://www.tpchd.org/sitemap-page-1.xml')\n",
    "sitemapxml = sitereq.text\n",
    "sitemapxml = sitemapxml.replace('ï»¿<?xml version=\"1.0\" encoding=\"UTF-8\"?><urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">', \"<urlset>\")\n",
    "# I had to do a replace because the xml header I was getting was throwing off the etree parser.\n",
    "# This scrubs some wierd, non-utf8 chars and simplifies the parent directory name.\n",
    "\n",
    "# Write xml to file\n",
    "f = open('sitemap.xml', 'w')\n",
    "f.write(sitemapxml)\n",
    "f.close()\n",
    "\n",
    "# Next, I create an xml ElementTree so I can easily iterate through.\n",
    "tree = etree.parse('sitemap.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# this will do the iterating, and will only save the URL to a list, the rest of the xml is not needed.   \n",
    "# The xml structure was a little wonky, so I append URL by element, not by iterating through <loc> tags.\n",
    "urllist = []\n",
    "score = []\n",
    "for i in range(len(root)):\n",
    "    urlname = root[i][0].text\n",
    "    urllist.append(urlname)\n",
    "\n",
    "#print (urllist)\n",
    "\n",
    "# Now, we iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object.\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text).\n",
    "# sleep() inserts a 1 to 3 second pause between requests to avoid the server shutting me out for spamming.\n",
    "for things in urllist:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \") #this pulls a list of all the Content Area widgets on a page.\n",
    "    pgtext = []\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar that prints a star every time a page is completed.\n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    else:\n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"ul\", \"ol\", \"li\"]) #this dives into each content area widget div to pull visible text\n",
    "            \n",
    "            for stuff in parsediv:\n",
    "                pgtext.append(stuff.text)  #this says that 'for each visible text element in each content widget on each page, save that text to a list'.\n",
    "        \n",
    "        #Now we transform the list into a text string to run it through the textstat module for flesch-kincaid level (otherwise, its just a list of html <tags>).\n",
    "        txtstr = \". \".join(pgtext)\n",
    "        \n",
    "        if(textstat.lexicon_count(txtstr)==0):  # another error handling routine, in case any div has characters, but none are words.\n",
    "            score.append('No content')\n",
    "        else:\n",
    "            \n",
    "            # catching other text string errors\n",
    "            while True:\n",
    "                try:\n",
    "                    rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "                    score.append(rdscore)\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    score.append('Content not text')    #<-----non-text content (nums) return TypeError\n",
    "                #except ZeroDivisionError:               \n",
    "                    #score.append('Content empty') <---ignore these, this was just a test item.\n",
    "\n",
    "# Next is a Pandas dataframe to organize our URL list and reading scores into a table.\n",
    "siteinventory = pandas.DataFrame(list(zip(urllist, score)), columns = ['URL', 'FK Reading Level'])\n",
    "\n",
    "# And then.... Save DataFrame to CSV.  Thats it!\n",
    "siteinventory.to_csv('siteinventory.csv')\n",
    "print ('Assessment of page readability is complete.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test box for finding readability\n",
    "\n",
    "I used a test code snippet so I didn't have to run through 300+ pages to troubleshoot the readability calculation module [textstat](https://pypi.python.org/pypi.textstat).  The URLs I tested were intentional, one was a control without a defined content area-the other was a complex page with content areas broken up into two separate divs that used H3, H2, P text tags and a table list (< tl >). All content showed up in the test print, so I am confident that multiple content divs will not break the function.\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Content', 9.1]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "from requests import get       # for getting sitemap xml, and calling HTML object from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas as pd            # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam the site with requests (can slow requests)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "url = ['https://www.tpchd.org/','https://www.tpchd.org/healthy-people/antibiotic-awareness']\n",
    "score = []\n",
    "\n",
    "# Iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text)\n",
    "for things in url:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \")\n",
    "    pgtext = []\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar that prints a star every time a page is completed.\n",
    "    \n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    else:\n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"ul\", \"ol\", \"li\"])\n",
    "            for stuff in parsediv:\n",
    "                pgtext.append(stuff.text)\n",
    "        \n",
    "        # then join the list into a text string and run it through the textstat module for flesch-kincaid level\n",
    "        txtstr = \" \".join(pgtext)\n",
    "        rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "        score.append(rdscore)\n",
    "        \n",
    "print(score)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell\n",
    "Want to view the specific text from a page that got a wierd readability score?  Just replace the url after \"url = \" with the destination page you wish to test, then click run. Make sure you leave the brackets and apostrophes ( ['...'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*['Tobacco products and e-cigarettes are not safe for youth.', 'No Resets is a multi-media campaign for youth and young adults intended to: ', '\\nIncrease awareness of risks associated with e-cigarettes and vapor products.\\nDecrease the number of young people who start using tobacco and vapor products.\\n', 'Increase awareness of risks associated with e-cigarettes and vapor products.', 'Decrease the number of young people who start using tobacco and vapor products.', 'You may not know:', '\\nE-cigarettes often contain nicotine, a toxic and addictive substance.\\nNicotine negatively affects brain development in youth.\\nE-cigarettes contain cancer-causing agents like formaldehyde, benzene, lead and nickel. \\nCompanies market e-cigarettes to youth.\\nThe e-liquid comes in many color and candy-like flavors that appeal to youth. (Bubble-gum, gummy bear, cotton candy etc.) \\n', 'E-cigarettes often contain nicotine, a toxic and addictive substance.', 'Nicotine negatively affects brain development in youth.', 'E-cigarettes contain cancer-causing agents like formaldehyde, benzene, lead and nickel. ', 'Companies market e-cigarettes to youth.', 'The e-liquid comes in many color and candy-like flavors that appeal to youth. (Bubble-gum, gummy bear, cotton candy etc.) ', 'Find out more about the campaign!', '\\nLearn facts about alternative tobacco products.\\nEarn promotional gear: T-shirts, ear buds, chargers, chapstick and more.\\nShare pictures and comments on our page.\\n', 'Learn facts about alternative tobacco products.', 'Earn promotional gear: T-shirts, ear buds, chargers, chapstick and more.', 'Share pictures and comments on our page.', 'Take part in the campaign:', '\\nDownload 11x17 campaign posters (Dragon, Magic, and Robots)\\nOrder posters. Email Jessica Alvestad at jalvestad@tphcd.org.\\nHost an event to promote the No Resets campaign at your organization.\\nPromote the campaign or take part in an event.\\n', 'Download 11x17 campaign posters (Dragon, Magic, and Robots)', 'Dragon', 'Magic', 'Robots', 'Order posters. Email Jessica Alvestad at jalvestad@tphcd.org.', 'jalvestad@tphcd.org', 'Host an event to promote the No Resets campaign at your organization.', 'Promote the campaign or take part in an event.', 'Resources', '\\nE-Cig and Vapor Toolkit—An Educational Guide for Prevention\\nCenters for Disease Control and Prevention\\nCampaign for Tobacco-Free Kids\\nThe Truth Campaign\\n', 'E-Cig and Vapor Toolkit—An Educational Guide for Prevention', 'E-Cig and Vapor Toolkit—An Educational Guide for Prevention', 'Centers for Disease Control and Prevention', 'Centers for Disease Control and Prevention', 'Campaign for Tobacco-Free Kids', 'Campaign for Tobacco-Free Kids', 'The Truth Campaign', 'The Truth Campaign', 'Questions?', 'Contact Jessica Alvestad at\\xa0jalvestad@tpchd.org\\xa0or (253) 377-4242.', 'jalvestad@tpchd.org', '\\r\\n\\t\\tFree viewers are required for some of the attached documents.They can be downloaded by clicking on the icons below.\\r\\n\\t', '\\nAcrobat Reader\\n\\n', '\\nFlash Player\\n\\n', '\\nWindows Media Player\\n\\n', '\\nMicrosoft Silverlight\\n\\n', '\\nWord Viewer\\n\\n', '\\nExcel Viewer\\n\\n', '\\nPowerPoint Viewer\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "from requests import get       # for getting sitemap xml, and calling HTML object from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas as pd            # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam the site with requests (can slow requests)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "urllist = ['https://www.tpchd.org/healthy-people/tobacco-free-living/no-resets-vape-is-no-game']\n",
    "score = []\n",
    "\n",
    "for things in urllist:\n",
    "    response = get(things)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \") #this pulls a list of all the Content Area widgets on a page.\n",
    "    pgtext = []\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar that prints a star every time a page is completed.\n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    else:\n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"a\", \"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"ul\", \"ol\", \"li\"]) #this dives into each content area widget div to pull visible text\n",
    "            \n",
    "            for stuff in parsediv:\n",
    "                pgtext.append(stuff.text)  #this says that 'for each visible text element in each content widget on each page, save that text to a list'.\n",
    "        \n",
    "        #Now we transform the list into a text string to run it through the textstat module for flesch-kincaid level (otherwise, its just a list of html <tags>).\n",
    "        txtstr = \". \".join(pgtext)\n",
    "        \n",
    "        if(textstat.lexicon_count(txtstr)==0):  # another error handling routine, in case any div has characters, but none are words.\n",
    "            score.append('No content')\n",
    "        else:\n",
    "            \n",
    "            # catching other text string errors\n",
    "            while True:\n",
    "                try:\n",
    "                    rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "                    score.append(rdscore)\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    score.append('Content not text')    #<-----non-text content (nums) return TypeError\n",
    "                #except ZeroDivisionError:               \n",
    "                    #score.append('Content empty') <---ignore these, this was just a test item.\n",
    "\n",
    "pgtextstr = []                \n",
    "\n",
    "for x in pgtext:\n",
    "    pgtextstr.append(x)\n",
    "\n",
    "print(pgtextstr[:])\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "clean = str(pgtextstr).replace(\"', '\",\" \").replace(\"\\n\", \"\")\n",
    "\n",
    "print(type(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
