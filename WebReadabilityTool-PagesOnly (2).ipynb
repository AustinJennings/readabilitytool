{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TGIS 501 Final Project - Web Scraping and Natural Language Processing\n",
    "\n",
    "In this project file, I use lxml, BeautifulSoup, Pandas, and textstat libraries to pull a list of all available page URLs in the domain www.tpchd.org, and then parse through the main content area of each and evaluate the reading level.  The end product is a csv file that contains the URL and reading level of all 330+ pages. \n",
    "<br>\n",
    "Runtime is about five and a half (5.5) minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************Assessment of page readability is complete.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "import requests                # for getting the XML sitemap from tpchd.org\n",
    "from requests import get       # for calling HTML objects from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas                  # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam server requests (can slow rate)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "\n",
    "# Open sitemap showing all pages from URL to xml\n",
    "sitereq = requests.request('GET', 'http://www.tpchd.org/sitemap-page-1.xml')\n",
    "sitemapxml = sitereq.text\n",
    "sitemapxml = sitemapxml.replace('ï»¿<?xml version=\"1.0\" encoding=\"UTF-8\"?><urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">', \"<urlset>\")\n",
    "# I had to do a replace because the xml header I was getting was throwing off the etree parser.\n",
    "# This scrubs some wierd, non-utf8 chars and simplifies the parent directory name.\n",
    "\n",
    "# Write xml to file\n",
    "f = open('sitemap.xml', 'w')\n",
    "f.write(sitemapxml)\n",
    "f.close()\n",
    "\n",
    "# Next, I create an xml ElementTree so I can easily iterate through.\n",
    "tree = etree.parse('sitemap.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# this will do the iterating, and will only save the URL to a list, the rest of the xml is not needed.   \n",
    "# The xml structure was a little wonky, so I append URL by element, not by iterating through <loc> tags.\n",
    "urllist = []\n",
    "score = []\n",
    "for i in range(len(root)):\n",
    "    urlname = root[i][0].text\n",
    "    urllist.append(urlname)\n",
    "\n",
    "#print (urllist)\n",
    "\n",
    "# Now, we iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object.\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text).\n",
    "# sleep() inserts a 1 to 3 second pause between requests to avoid the server shutting me out for spamming.\n",
    "for things in urllist:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \") #this pulls a list of all the Content Area widgets on a page.\n",
    "    pgtext = []\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar that prints a star every time a page is completed.\n",
    "    \n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    #this says that 'for each visible text element in each content widget on each page, save that text to a list'.\n",
    "    else: \n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"td\", \"li\"])\n",
    "            \n",
    "            for stuff in parsediv:\n",
    "                if stuff.name ==\"li\":\n",
    "                    pgtext.append(stuff.text+\".\")\n",
    "\n",
    "                else:\n",
    "                    pgtext.append(stuff.text) \n",
    "        \n",
    "        #Now we transform the list into a text string to run it through the textstat module for flesch-kincaid level (otherwise, its just a list of html <tags>).\n",
    "        txtstr = \" \".join(pgtext[0:-1])\n",
    "        \n",
    "        if(textstat.lexicon_count(txtstr)==0):  # another error handling routine, in case any div has characters, but none are words.\n",
    "            score.append('No content')\n",
    "        else:\n",
    "            \n",
    "            # catching other text string errors\n",
    "            while True:\n",
    "                try:\n",
    "                    rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "                    score.append(rdscore)\n",
    "                    break\n",
    "                except TypeError:\n",
    "                    score.append('Content not text')    #<-----non-text content (nums) return TypeError\n",
    "                #except ZeroDivisionError:               \n",
    "                    #score.append('Content empty') <---ignore these, this was just a test item.\n",
    "\n",
    "# Next is a Pandas dataframe to organize our URL list and reading scores into a table.\n",
    "siteinventory = pandas.DataFrame(list(zip(urllist, score)), columns = ['URL', 'FK Reading Level'])\n",
    "\n",
    "# And then.... Save DataFrame to CSV.  Thats it!\n",
    "siteinventory.to_csv('siteinventory.csv')\n",
    "print ('Assessment of page readability is complete.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test box for finding readability\n",
    "\n",
    "I used a test code snippet so I didn't have to run through 300+ pages to troubleshoot the readability calculation module [textstat](https://pypi.python.org/pypi.textstat).  The URLs I tested were intentional, one was a control without a defined content area-the other was a complex page with content areas broken up into two separate divs that used H3, H2, P text tags and a table list (< tl >). All content showed up in the test print, so I am confident that multiple content divs will not break the function.\n",
    "\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No Content', 9.1]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "from requests import get       # for getting sitemap xml, and calling HTML object from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas as pd            # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam the site with requests (can slow requests)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "url = ['https://www.tpchd.org/','https://www.tpchd.org/healthy-people/antibiotic-awareness']\n",
    "score = []\n",
    "\n",
    "# Iterate through URL list, request each HTML with get(), pass HTML to BeautifulSoup object\n",
    "# Once we have the BS object, search find all <div> elements with the content class we are looking for (main text)\n",
    "for things in url:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \")\n",
    "    pgtext = []\n",
    "    print(\"*\", end=\"\", flush=True) #makeshift status bar that prints a star every time a page is completed.\n",
    "    \n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    else:\n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"ul\", \"ol\", \"li\"])\n",
    "            for stuff in parsediv:\n",
    "                pgtext.append(stuff.text)\n",
    "        \n",
    "        # then join the list into a text string and run it through the textstat module for flesch-kincaid level\n",
    "        txtstr = \" \".join(pgtext)\n",
    "        rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "        score.append(rdscore)\n",
    "        \n",
    "print(score)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell\n",
    "Want to view the specific text from a page that got a wierd readability score?  Just replace the url after \"url = \" with the destination page you wish to test, then click run. Make sure you leave the brackets and apostrophes ( ['...'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity. Respect.\n",
      "[14.7]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  # for parsing html objects to find our content area of interest\n",
    "from requests import get       # for getting sitemap xml, and calling HTML object from the web into BeautifulSoup\n",
    "from lxml import etree         # for parsing the xml to extract the URL list\n",
    "import pandas as pd            # for zipping lists into an easy-to-use, exportable data frame\n",
    "import csv                     # for delivering the final output in an excel-fiendly format\n",
    "from time import sleep         # in case tpchd.org gets mad that we spam the site with requests (can slow requests)\n",
    "from random import randint     # in case we need to slow requests on random intervals to mimmick human behavior\n",
    "\n",
    "from textstat.textstat import textstat  # this magical module calculates readability in exactly the index I need\n",
    "\n",
    "\n",
    "url = ['https://www.tpchd.org/i-want-to-/about-us/about-the-health-department/values']  #<-----------Here's where you put your test URL\n",
    "\n",
    "score = []\n",
    "\n",
    "\n",
    "for things in url:\n",
    "    response = get(things)\n",
    "    text = response.text\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    test = soup.find_all('div',class_=\"content_area normal_content_area clearfix \")\n",
    "    pgtext = []   \n",
    "    if(len(test) == 0):\n",
    "        score.append('No Content')\n",
    "    else:\n",
    "        for tag in test:\n",
    "            parsediv = tag.find_all([\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"td\", \"li\"])\n",
    "            \n",
    "            for stuff in parsediv:\n",
    "                if stuff.name ==\"li\" or \"h1\":\n",
    "                    pgtext.append(stuff.text+\".\")\n",
    "\n",
    "                else:\n",
    "                    pgtext.append(stuff.text)\n",
    "\n",
    "txtstr = \" \".join(pgtext[0:-1])\n",
    "rdscore = textstat.flesch_kincaid_grade(txtstr)\n",
    "score.append(rdscore)\n",
    "        \n",
    "print(txtstr)\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
